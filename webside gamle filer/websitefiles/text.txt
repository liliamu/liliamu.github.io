Moving through a tunnel, the viewer is recorded and transformed to an abstraction of the Nordic landscapes  - represented by computer vision. The underlying system uses a deep learning method called style transfer to distinguish semantic content from texture in an image. The method uses convolutional neural networks to learn a representation of the style of an image, and apply it to a different content image - in this case a real time video feed. This project explores computational creativity and computers as a visual system. It questions how we can view artificially intelligent entities as more than tools in artistic practice.

Awarded status as national eScience pilot project, by the Danish e-Infrastructure Cooperation (DeIC). Exhibited at Click festival, 2017.

		The project This project was part of my master thesis (“ART+AI: exploring new approaches in creative practice”), where the goal was to use AI as a generative visualisation technique. The creative production consisted of developing, designing, and implementing the installation, including the architectural structure, and underlying system for visualisation. The exhibition was done in collaboration with CATCH (Center for Art +Tech),  for Click festival, and showcased from May 20th to 21st,  2017, in Kulturværftet, Helsingør. 	The systemThe installation uses the neural style transfer method, a computer vision technique that utilises deep learning and neural networks, to perform texture synthesis. This technique generates a new image, by re-rendering the content from an arbitrary image, in the style of a different image. This installation re-renders a real time video feed, in the textures of a natural element. This happens by the network training on photographs of said element, and learning how the “style” is represented in pixel space. Training was done one the Abacus supercomputer (via DeIC grant). The real time performance is achieved via a custom-made script, developed together with Ole Kristensen. The style transfer technique is implemented via Logan Engstrom’s Tensorfow approach. The installation runs on an openFrameworks application, developed specifically for the project. This application is now available as open source code, (faster-style @ github), and included in the openFrameworks Tensorflow add-on, with a speed of 9 fps on a Nvidia GeForceGTX 1080.	The structure The structure was designed in collaboration with Margrete Bjone Engelien, architect. The structural design allows for a space where the viewer can move and interact with the installation with their whole body. The encompassing tunnel shape, also allows for the viewer to go inside and be hidden, creating a more isolated experience.  	“ The installation has a familiarity in seeing oneself at the other side of the tunnel, where the real-time effect acts as an immediate confirmation that the viewer’s movement creates changes in the visualisation. Yet there is an uncanniness in looking at oneself in a distorted machine vision “mirror”, merged in an almost-familiar texture, as nature seen by a machine. This parallel draws the viewer to interact with the installation by playing, exploring, and testing a threshold of the familiar and the uncanny. ”